---
title: 学习笔记：爬虫与反爬
tags: [wiki,笔记整理]
---
出书的人和经验丰富的实战工程师区别大概就在于此。
[参考链接](https://blog.csdn.net/fei2636/article/details/78999318)
<!-- more -->
# 爬虫与反爬概述
就像攻击武器与防御武器一样，双方总是在不断升级。爬虫和反爬是典型的攻防双方的互相升级。但这种升级不像军事，军事是无尽头的，但是爬虫和反爬是有尽头的。
## 爬虫的尽头
就是浏览器，一旦使用浏览器，程序完全可以模拟真实用户发出请求，
### 缺点
消耗资源，因为需要新开一个进程，解析DOM，运行客户端JavaScript代码。（[chrome的node api](https://github.com/GoogleChrome/puppeteer)在github开源仅仅两天，就拿到8k个star）
## 反爬的尽头
就是像Google这种超级厉害的验证码，毕竟验证码的根本目的就是识别人类和机器的。
## 职业道德
成规模的爬虫一般都会使用集群，一般的小网站服务器规模可能不如爬虫集群的规模大。所以很多时候我们最好对要爬的网站限制一下频率。否则这些爬虫就相当于DoS攻击集群了！一般的网站都会有robots.txt可以参考。
# 爬虫与反爬一览
反爬|应对
--|--
频率限制|随机sleep
登陆限制|加上cookie
header|header池
JS|[js反爬](#JavaScript脚本动态获取网站数据)
验证码|机器学习
ip限制|代理池和高匿代理等好用的东西
内容反爬|OCR
# 常见的反爬措施有下面几种：
## 访问频率

很好理解，如果访问太频繁网站可能针对你的ip封锁一段时间，这和防DDoS的原理一样。对于爬虫来说，碰到这样的限制一下任务的频率就可以了，可以尽量让爬虫想人类一样访问网页（比如随机sleep一段时间，如果每隔3s访问一次网站很显然不是正常人的行为）。
## 登录限制

也比较常见。不过公开信息的网站一般不会有这个限制，这样让用户也麻烦了。其实反爬措施都或多或少的影响真实用户，反爬越严格，误杀用户的可能性也越高。对爬虫来说，登录同样可以通过模拟登录的方式解决，加个cookie就行了（话又说回来，网络的原理很重要）。
## 通过Header封杀

一般浏览器访问网站会有header，比如Safari或者Chrome等等，还有操作系统信息。如果使用程序访问并不会有这样的header。破解也很简单，访问的时候加上header就行。
## JavaScript脚本动态获取网站数据

有一些网站（尤其是单页面网站）的内容并不是通过服务器直接返回的，而是服务器只返回一个客户端JavaScript程序，然后JavaScript获取内容。更高级的是，JavaScript在本地计算一个token，然后拿这个token来进行AJAX获取内容。而本地的JavaScript又是经过代码混淆和加密的，这样我们做爬虫的通过看源代码几乎不可能模拟出来这个请求（主要是token不可能破解），但是我们可以从另一个角度：headless的浏览器，也就是我们直接运行这个客户端程序，这可以100%地模拟真实用户！
## 验证码

这几乎是终极武器了，验证码是专门用来区分人和计算机的手段。对于反爬方来说，这种方式对真实用户和搜索引擎（其实可以通过记录搜索引擎爬虫的ip来区别对待，可以解决）的危害比较大，相信读者都有输入验证码的痛苦经历。但这种方法也并不是无敌的！通过现在很火的机器学习可以轻松的识别大部分的验证码！Google的reCAPTCHA是一种非常高级的验证码，但是听过通过模拟浏览器也是可以破解的。有的网站需要验证码验证拿到一个token，token长得很像一个时间戳，本地自己生成一个时间戳发现也是能用的！于是就这样绕过了验证码。
## ip限制

网站可能将识别的ip永久封杀，这种方式需要的人力比较大，而且误伤用户的代价也很高。但是破解办法却非常简单。目前代理池几乎是搞爬虫的标配了，甚至还有很多高匿代理等好用的东西。所以这基本上只能杀杀小爬虫。
## 网站内容反爬

有一些网站将网站内容用只有人类可以接收的形式来呈现（其实反爬就是区别对待人类和机器嘛）。比如将内容用图片的形式显示。但是近几年来人类和机器的差别越来越小，图片可以用OCR准确率非常高地去识别。



